---
title: Configuring Loki on OCP
index: 03
---
# Configuring Loki

## About
Grafana Loki is a solution that covers the entire logging stack following the success story of Prometheus and how this technology optimises the metrics transmission and indexing process. Compared to ELK, Loki uses a lightweight indexing mechanism which groups log entries into streams and tags them with a particular set of labels and values. The log entries are stored as plaintext (in raw format) and are further identified by the subset of labels corresponding to their origin (eg: env=production, app=frontend). Particularly for Kubernetes, source tags can often correspond to the labelling convention used within the definition of the pods.

The indexing mechanism becomes effective when the logs have to be consumed aggressively and they have to be available as quick as possible. Loki does not iterate through the log content in order to generate the index map, therefore, no further content analyses will be performed. Scenarios similar to the following: _I need to capture all the HTTP 50* errors from my front-end web servers, occurred during a POST form submission_ may better fit solutions like ELK where the platform establish indexing based on the content analysis.

## Architectural Overview
```
                             [FRONTEND]
                                 |___[HTTP/GET(query_range)]____
                                                                |
                  _________________[HTTP/POST]____________ 3100/tcp[LOKI]
                  |             |                   |
              [PROMTAIL i1] [PROMTAIL i2]  .... [PROMTAIL iN]
              __________      __________         __________
               [NODE 1]        [NODE 2]           [NODE N]
```
![Architectural Overview](./visuals/loki4bmrg.png)

## Prerequisites

|  Chart | Version  |
|---|---|
|grafana/loki  |  2.5.0  |
|grafana/promtail | 3.5.0   |


## Installing

**Step 0**
Create a dedicated namespace for the loki stack
```
kubectl create ns bmrg-loki
```

**Step 1**

Add the official Grafana Labs repository to the helm3 repo list
```
helm repo add grafana https://grafana.github.io/helm-charts
```

**Step 2**

Generate the values files corresponding to the charts versions listed in prerequisites
```
helm inspect values --version 2.5.0 grafana/loki > loki-values-v2.5.0.deploy.yaml
helm inspect values grafana/promtail --version 3.5.0 > promtail-values-v3.5.0.deploy.yaml
```

Aspects to be considered:
- activate persistence within the loki chart values if enforcing the retention policy
- make sure the connection coordinates towards loki are set correctly in the promtail chart values
```
config.lokiAddress: http://bmrg-loki:3100/loki/api/v1/push
```

- use the `scrapeConfig` defined in [Scrape Configurations](#scrape-configurations)
- configure the pipeline based on the containers engine in use as defined in [Pipeline Stages](#pipeline-stages)
- if using an **Openshift** flavor, remember that the Pod Security Policies admission controller has been replaced by the Security Context Constraints admission controller. In this regard, extra steps must be performed:
  - escalate promtail privileges
    ```
    containerSecurityContext:
      privileged: true
      readOnlyRootFilesystem: true
      capabilities:
        drop:
          - ALL
      allowPrivilegeEscalation: true
    ```

  - create a service account and link the `privileged` and `hostmount-anyuid` SCC to it

    Values file:
    ```
    serviceAccount:
      # -- Specifies whether a ServiceAccount should be created
      create: true
      # -- The name of the ServiceAccount to use.
      # If not set and `create` is true, a name is generated using the fullname template
      name: bmrg-promtail-sa
      # -- Image pull secrets for the service account
      imagePullSecrets: []
      # -- Annotations for the service account
      annotations: {}
    ```

    K8S objects file (use `oc apply -f <file name>`):
    ```
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      creationTimestamp: null
      name: promtail-role
    rules:
    - apiGroups:
      - security.openshift.io
      resourceNames:
      - privileged
      - hostmount-anyuid
      resources:
      - securitycontextconstraints
      verbs:
      - use

    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      creationTimestamp: null
      name: promtail-priv
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: promtail-role
    subjects:
    - kind: ServiceAccount
      name: bmrg-promtail-sa
      namespace: bmrg-loki
    ```

  - in the Loki values file disable the security context, as Openshift has it's own mechanism to manage it
     ```
     securityContext:
        fsGroup: null
        runAsGroup: null
        runAsNonRoot: true
        runAsUser: null
     ```
     ```
     serviceAccount:
        create: false
        name:
        annotations: {}
     ```

**Step 3**

Install Loki (ingestion service) and wait for the service to run:
```
helm3 upgrade --install  --debug --namespace bmrg-loki -f loki-values-v2.5.0.deploy.yaml --version 3.5.0 bmrg-promtail grafana/promtail
```

**Step 4**

Install Promtail daemonset (agent):
```
helm3 upgrade --install  --debug --namespace bmrg-loki -f promtail-values-v3.5.0.deploy.yaml --version 3.5.0 bmrg-promtail grafana/promtail
```

Remarks:
- due to the tight scraping configuration, adapted to capture logs only from flow & cicd jobs, some promtail agents may be in an error state failing the readiness probe (0/1 READY). If the promtail pod logs display a message similar to the one below, the error can be disregarded.
```
level=warn ts=2021-03-13T16:40:59.636745016Z caller=logging.go:62 msg="GET /ready (500) 56.509Âµs Response: \"Not ready: Unable to find any logs to tail. Please verify permissions, volumes, scrape_config, etc.\\n\" ws: false; Accept-Encoding: gzip; Connection: close; User-Agent: kube-probe/1.13; "
```

  this particular error says that no active pod, running on the target node, matches the scrape configuration. In other words, no flow or cicd job has been scheduled on the target node.

## Technical details

### Retention policy
```
config:
  auth_enabled: false
  [...]
  limits_config:
    enforce_metric_name: false
    reject_old_samples: true
    reject_old_samples_max_age: 168h
  schema_config:
  [...]
  server:
    http_listen_port: 3100
  storage_config:
  [...]
  chunk_store_config:
    max_look_back_period: 168h
  table_manager:
    retention_deletes_enabled: true
    retention_period: 168h
  [...]
```
### Pipeline Stages
Depending on the container engine used by the Kubernetes flavor, the logs format may differ and therefore, in order to retrieve relevant information, the pipeline stages must be configured according to the format
#### Docker
```
config:
[...]
  snippets:
    pipelineStages:
      - json:
          expressions:
            output: log
      - output:
            source: output
```
#### CRI-O
```
config:
[...]
  snippets:
    pipelineStages:
      - regex:
          expression: "^(?s)(?P<time>\\S+?) (?P<stream>stdout|stderr) (?P<flags>\\S+?) (?P<content>.*)$"
      - output:
          source: content
      - replace:
          expression: "()\\z"
          replace: " \n"
```

### Scrape Configurations
```
scrapeConfigs: |
  - job_name: bmrg-job
    kubernetes_sd_configs:
    - role: pod
    pipeline_stages:
    {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}

    relabel_configs:
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_workflow_id
      target_label: bmrg_workflow
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_activity_id
      target_label: bmrg_activity
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_task_id
      target_label: bmrg_task
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_workflow_activity_id
      target_label: bmrg_workflow_activity
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_task_activity_id
      target_label: bmrg_task_activity
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_tier
      target_label: __bmrg_tier__
    - source_labels:
      - __meta_kubernetes_pod_label_boomerang_io_product
      target_label: bmrg_product
    - source_labels:
      - __meta_kubernetes_pod_container_name
      target_label: bmrg_container
    - source_labels:
      - __meta_kubernetes_pod_node_name
      target_label: __host__
    - action: drop
      regex: ""
      source_labels:
      - bmrg_workflow_activity
    - action: drop
      regex: ""
      source_labels:
      - bmrg_task_activity
    - action: drop
      regex: ""
      source_labels:
      - bmrg_workflow
    - action: drop
      regex: ""
      source_labels:
      - bmrg_task
    - action: keep
      regex: task-cntr|worker-cntr|step-[a-zA-Z0-9][a-zA-Z0-9_-]*
      source_labels:
      - bmrg_container
    - action: keep
      regex: bmrg-flow|bmrg-cicd-flow
      source_labels:
      - bmrg_product
    {{- toYaml .Values.config.snippets.common | nindent 2 }}
```

Log file discovery:
```
config:
[...]
  snippets:
    common:
    - replacement: /var/log/pods/*$1/*.log
      separator: /
      source_labels:
      - __meta_kubernetes_pod_uid
      - __meta_kubernetes_pod_container_name
      target_label: __path__
```
## Node affinity and toleration when using dedicated nodes
```
affinity:
  nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: node-role.kubernetes.io/task-worker
            operator: Exists
```

```
tolerations:
  - key: dedicated
    operator: Equal
    effect: NoSchedule
    value: "bmrg-worker"
```
## References
* Official Loki website: [https://grafana.com/oss/loki/](https://grafana.com/oss/loki/)
* Official Loki Github Repository: [https://github.com/grafana/loki](https://github.com/grafana/loki)
* Loki Helm Chart: [https://github.com/grafana/helm-charts/tree/main/charts/grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana)
